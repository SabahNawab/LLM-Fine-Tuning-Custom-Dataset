{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3a469b9",
      "metadata": {
        "id": "c3a469b9"
      },
      "source": [
        "IMPORTING REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COLAB version\n",
        "# %%capture\n",
        "# !pip install -U datasets\n",
        "# !pip install -U accelerate\n",
        "# !pip install -U peft\n",
        "# !pip install -U trl\n",
        "# !pip install -U bitsandbytes\n",
        "# !pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
        "# !pip install -q unsloth\n",
        "# !pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "# !pip install unsloth_zoo"
      ],
      "metadata": {
        "id": "XCxenBVxQauI"
      },
      "id": "XCxenBVxQauI",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ea508693",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea508693",
        "outputId": "c2d8a3a2-2fca-4c16-b483-3ca88eb5f5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "import dotenv\n",
        "from unsloth import FastModel\n",
        "from datasets import load_dataset\n",
        "from transformers import TextStreamer, AutoProcessor\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "from unsloth.chat_templates import get_chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de480789",
      "metadata": {
        "id": "de480789"
      },
      "source": [
        "HF AUTHENTICATION FOR GATED MODELS LIKE GEMMA (NEEDS ACCESS PERMISSIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2df38545",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "2df38545"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"]=dotenv.get_key(\".env\", \"HF_TOKEN\")\n",
        "hf_token=os.environ[\"HF_TOKEN\"]\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5e8859",
      "metadata": {
        "id": "bf5e8859"
      },
      "source": [
        "MODEL & TOKENIZER LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a3349dc7",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "7e7555b91f99466ba04959da4c0c85ab",
            "7a58dcffa47343e1b3c90175ca378a97",
            "a587f412fc3a4e91a1bd817c044ba1ef",
            "f6fa098f903c484c81d7c1bc08be483b",
            "ca38d2313db24c3cb851274083cc88cc",
            "9a3ca54162984395b6d1b53e598def8e",
            "0467899d39c741f6ab2dfd3ca786fe58",
            "03da635f7e2342629e70ef2f9f85d546",
            "9c0a46e6d69c430baaeb173f629162a4",
            "f8e4472105b943908c1d8011c3337462",
            "481bf2121cdb4c80b02291388b693e23"
          ]
        },
        "id": "a3349dc7",
        "outputId": "69b5cdf3-9905-4337-9618-870d7ab26663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.9: Fast Gemma3 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e7555b91f99466ba04959da4c0c85ab"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 1024,\n",
        "    load_in_4bit = False,\n",
        "    load_in_8bit = True,\n",
        "    full_finetuning = False,\n",
        "    token=hf_token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1596403f",
      "metadata": {
        "id": "1596403f"
      },
      "source": [
        "LOADING & FORMATTING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3fbbd903",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fbbd903",
        "outputId": "01a1a966-4f36-4d98-d98d-07547c9dc637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'output'],\n",
            "    num_rows: 599\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "local_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"data.jsonl\",\n",
        "    split=\"train\"\n",
        "   )\n",
        "\n",
        "print(local_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "97958de3",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "28e188953df345109b85406736c8981f",
            "6f753b033f67456d9921bc7dfa64ef46",
            "8e00b86a11a2470e97616f9b8bcfb10d",
            "47fe55ed5b8f43cdae6e3a9cd183dd87",
            "2b6bfb583f5b432081a1dec7073bcf2f",
            "0294ad32b014460baa88ac2b8ce46006",
            "66de3bd2854d4c4a92daea3fbc25b88f",
            "2583f7e269c34fa19fdc0f0cba1e9fc1",
            "50ac8e67606a43be81695d14235a9d9b",
            "796cb2338c8d46b9b0671faba72c672d",
            "2a484eb38208409f948808ec9b09c866"
          ]
        },
        "id": "97958de3",
        "outputId": "9ae667f3-b48b-47c5-f36e-2908d31d79e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28e188953df345109b85406736c8981f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Where can I find the best Chapli Kebab in Peshawar?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Rora, this is the most important question! There are two camps. For the absolute classic, you go to **Jalil Kabab House** in Firdous\u2014it's iconic. But if you want the rustic vibe, head to **Taru Jabba** outside the city. Just don't ask for a menu, just say 'special' and enjoy.<end_of_turn>\n",
            "<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "\n",
        "def format_instruction(example):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"model\", \"content\": example[\"output\"]},\n",
        "    ]\n",
        "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)+ EOS_TOKEN}\n",
        "\n",
        "dataset = local_dataset.map(format_instruction)\n",
        "\n",
        "# You can print an example to verify\n",
        "print(dataset[0]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed7c3aee",
      "metadata": {
        "id": "ed7c3aee"
      },
      "source": [
        "RESPONSES OF BASE INSTRUCT MODEL BEFORE FINETUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "12aeb64c",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12aeb64c",
        "outputId": "be9f1f71-2724-477f-cb2d-a50aa47554af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            ">>> GENERATION BEFORE FINE-TUNING (General Model) <<<\n",
            "==================================================\n",
            "<bos><bos><start_of_turn>user\n",
            "Where can I find the best Chapli Kebab in Peshawar?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Okay, you're asking about the holy grail of Peshawar food \u2013 Chapli Kebab! It's a serious topic here. While many places claim to have the \"best,\" here's a breakdown of the top contenders and what makes them special, categorized for clarity:\n",
            "\n",
            "**1. The Absolute Legends (Most Frequently Recommended & Often Considered the Best):**\n",
            "\n",
            "* **Chapli Kababi:** (Multiple Locations - Most Popular: University Road) - This is *the* place everyone points to. They've been doing it for decades and perfected the recipe.\n",
            "    * **Why it's top-tier:** Their Chapli Kebab is incredibly juicy, flavorful, and has a perfect balance of spices. They use a unique blend of spices, including a generous amount of chili powder, giving it that signature red color and heat.  Their chutney is legendary \u2013 tangy, sweet, and spicy.\n",
            "    * **Location:** University Road (main branch), and several other locations across Peshawar (check Google Maps for the nearest).\n",
            "    * **Price:** Around PKR 300-500 per Chapli Kebab.\n",
            "    * **Website:** [https://chaplikababi.com/](https://chaplikababi.com/)\n",
            "\n",
            "\n",
            "* **Qishmish Chapli Kebab:** (University Road) - A strong contender and a close second to Chapli Kababi.\n",
            "    * **Why it's top-tier:**  Their Chapli Kebab is known for being exceptionally tender and flavorful. They often use a slightly different spice blend, which some people prefer.  Their service is also generally excellent.\n",
            "    * **Location:** University Road.\n",
            "    * **Price:** Similar to Chapli Kababi \u2013 PKR 300-500.\n",
            "\n",
            "**2. Excellent Options - Worth a Try:**\n",
            "\n",
            "* **Shahana Restaurant:** (University Road) \u2013 A popular and well-established restaurant with a good Chapli Kebab option.\n",
            "    * **Why it\u2019s good:** They've been around for a long time and have a consistent quality.  It\u2019s a good, solid choice if you want a reliable Chapli Kebab experience.\n",
            "    * **Location:** University Road.\n",
            "    * **Price:** PKR 250-400.\n",
            "\n",
            "* **Dukhi Chapli Kebab:** (Saifullah Market) - A more local, less touristy option that's gaining popularity.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\">>> GENERATION BEFORE FINE-TUNING (General Model) <<<\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "FastModel.for_inference(model)\n",
        "\n",
        "test_instruction = \"Where can I find the best Chapli Kebab in Peshawar?\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": test_instruction},\n",
        "]\n",
        "\n",
        "prompt_string = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    tokenize = False,\n",
        ")\n",
        "\n",
        "inputs = tokenizer(text = [prompt_string], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 512,\n",
        "    temperature = 0.7,\n",
        "    do_sample = True,\n",
        "    use_cache = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7ead5c",
      "metadata": {
        "id": "6c7ead5c"
      },
      "source": [
        "PARAMETER EFFICIENT FINE-TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "fd4f16dd",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd4f16dd",
        "outputId": "7d7adb27-f175-4e32-a29c-5605dae06731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    finetune_vision_layers = False, # Turn off for just text!\n",
        "    finetune_language_layers = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules = True,  # Should leave on always!\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8a2a9be2",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "57d5c4f39ca345aa8def5ebcdcb11d42",
            "9d3a997b5c9740428f74c0a6d950029c",
            "6c1745d9b12344fd888d118c8239f67c",
            "741121bc6f514e83bfd93b49a73bbbfd",
            "a3a24d2fb5a5487db4c55164dd74bf9e",
            "386bf363fee04b46a1b03672848b957c",
            "ef4026a00511466782331d25701c75f0",
            "47c6f49f497740558835a5156aeaff6a",
            "8705779aca8c4c27b282bdfc1031b522",
            "73dada7c446044578d04478cde2a9024",
            "8275ea1caf564f5692e3f12574b865f8"
          ]
        },
        "id": "8a2a9be2",
        "outputId": "878603ad-e9c5-4f02-be31-a20d71e79c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=1):   0%|          | 0/599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57d5c4f39ca345aa8def5ebcdcb11d42"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 512,\n",
        "    dataset_num_proc = 2,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 20,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c11a73b4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "c11a73b4",
        "outputId": "39ea6ff5-88cf-4485-c226-ca22cd702408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 599 | Num Epochs = 3 | Total steps = 225\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 32,788,480 of 4,332,867,952 (0.76% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/225 19:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.495400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.214000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.955100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.825100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.532300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.455700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.289900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.058200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.039100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.016300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=225, training_loss=1.8324704848395454, metrics={'train_runtime': 1167.7204, 'train_samples_per_second': 1.539, 'train_steps_per_second': 0.193, 'total_flos': 2224548882454080.0, 'train_loss': 1.8324704848395454, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c511e97d",
      "metadata": {
        "id": "c511e97d"
      },
      "source": [
        "INFERENCE AFTER FINE-TUNING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FastModel.for_inference(model)\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "\n",
        "test_cases = [\n",
        "    {\"q\": \"where can I find the best chapli kebab in peshawar?\", \"temp\": 0.7},\n",
        "    {\"q\": \"Hello, How are you doing today?\", \"temp\": 0.8},\n",
        "    {\"q\": \"where should I go for good food in peshawar?\", \"temp\": 0.4}\n",
        "]\n",
        "\n",
        "\n",
        "for i, case in enumerate(test_cases, 1):\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\">>> TEST OUTPUT {i} (Temp: {case['temp']})\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": case[\"q\"]}]\n",
        "\n",
        "    prompt_string = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = True,\n",
        "    )\n",
        "\n",
        "\n",
        "    inputs = tokenizer(text = [prompt_string], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "    _ = model.generate(\n",
        "        **inputs,\n",
        "        streamer = text_streamer,\n",
        "        max_new_tokens = 512,\n",
        "        temperature = case[\"temp\"],\n",
        "        do_sample = True,\n",
        "        use_cache = True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3F8Go7Xkb_g",
        "outputId": "ce6039b2-8310-454c-8cf8-974371c8c851"
      },
      "id": "v3F8Go7Xkb_g",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            ">>> TEST OUTPUT 1 (Temp: 0.7)\n",
            "==============================\n",
            "<bos><bos><start_of_turn>user\n",
            "where can I find the best chapli kebab in peshawar?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Salam. You have to go to **Chowk Yadgar** in the middle of the night. The street vendors fire the meat on charcoal, so it is crispy and juicy. Don't ask for a bun, eat it with bread. You will love it. Enjoy the feast.<end_of_turn>\n",
            "\n",
            "==============================\n",
            ">>> TEST OUTPUT 2 (Temp: 0.8)\n",
            "==============================\n",
            "<bos><bos><start_of_turn>user\n",
            "Hello, How are you doing today?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Shukar de, I am fit. Ready for your command.<end_of_turn>\n",
            "\n",
            "==============================\n",
            ">>> TEST OUTPUT 3 (Temp: 0.4)\n",
            "==============================\n",
            "<bos><bos><start_of_turn>user\n",
            "where should I go for good food in peshawar?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Salam Boss! For authentic Peshawari food, head to **Saddar**. Try the **Nihari** or **Korma** at a local spot. If you want a family vibe, go to **Mardan Sweets** for the sweet and salty mix. Eat like a local! Manana.<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7c7412",
      "metadata": {
        "id": "ee7c7412"
      },
      "source": [
        "MODEL SAVING AND RUN THROUGH OLLAMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "458c655b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "458c655b",
        "outputId": "4087f98e-7ce8-4951-9447-a89f0e2210cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['peshawari_lora/processor_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "model.save_pretrained(\"peshawari_lora\")\n",
        "tokenizer.save_pretrained(\"peshawari_lora\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_text, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"peshawari_lora\", # Load your trained adapters\n",
        "    max_seq_length = 1024,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "\n",
        "model_text.save_pretrained_gguf(\"actual_ai_gguf\", tokenizer, quantization_method = \"q4_k_m\")"
      ],
      "metadata": {
        "id": "TqO3-3dBv5uS"
      },
      "id": "TqO3-3dBv5uS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run command in the terminal : ollama create actual_ai -f Modelfile\n"
      ],
      "metadata": {
        "id": "B9jXYbGvtA3G"
      },
      "id": "B9jXYbGvtA3G"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}